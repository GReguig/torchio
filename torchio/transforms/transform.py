import numbers
from typing import Union
from copy import deepcopy
from abc import ABC, abstractmethod

import torch
import SimpleITK as sitk

from .. import TypeData, INTENSITY, DATA
from ..data.image import Image
from ..data.subject import Subject
from ..data.dataset import ImagesDataset
from ..utils import nib_to_sitk, sitk_to_nib
import numpy as np
import time

class Transform(ABC):
    """Abstract class for all TorchIO transforms.

    All classes used to transform a sample from an
    :py:class:`~torchio.ImagesDataset` should subclass it.
    All subclasses should overwrite
    :py:meth:`torchio.tranforms.Transform.apply_transform`,
    which takes a sample, applies some transformation and returns the result.

    Args:
        p: Probability that this transform will be applied.
    """
    def __init__(self, p: float = 1, verbose: bool = False, keep_original=False):
        self.probability = self.parse_probability(p)
        self.verbose = verbose
        self.keep_original = keep_original

    def __call__(self, data: Union[Subject, torch.Tensor]):
        """Transform a sample and return the result.

        Args:
            data: Instance of :py:class:`~torchio.Subject` or 4D
                :py:class:`torch.Tensor` with dimensions :math:`(C, D, H, W)`,
                where :math:`C` is the number of channels and :math:`D, H, W`
                are the spatial dimensions. If the input is a tensor, the affine
                matrix is an identity and a tensor will be also returned.
        """
        if isinstance(data, torch.Tensor):
            is_tensor = True
            sample = self.parse_tensor(data)
        else:
            is_tensor = False
            sample = data

        if self.keep_original:
            for image_name, image_dict in sample.get_images_dict().items():
                new_key = image_name + '_orig'
                if new_key not in sample:
                    sample[new_key] = dict(data=image_dict['data'], type='original', affine=image_dict['affine'])

        if self.verbose:
            start = time.time()

        self.parse_sample(sample)
        if torch.rand(1).item() > self.probability:
            return sample
        sample = deepcopy(sample)
        transformed = self.apply_transform(sample)
        if self.verbose:
            duration = time.time() - start
            print(f'{self.__class__.__name__}: {duration:.3f} seconds')

        if is_tensor:
            num_channels = len(data)
            images = [
                transformed[f'channel_{i}'][DATA]
                for i in range(num_channels)
            ]
            transformed = torch.cat(images)
        return transformed

    @abstractmethod
    def apply_transform(self, sample: Subject):
        raise NotImplementedError

    @staticmethod
    def parse_probability(probability: float) -> float:
        is_number = isinstance(probability, numbers.Number)
        if not (is_number and 0 <= probability <= 1):
            message = (
                'Probability must be a number in [0, 1],'
                f' not {probability}'
            )
            raise ValueError(message)
        return probability

    @staticmethod
    def parse_sample(sample: Subject) -> None:
        if not isinstance(sample, Subject) or not sample.is_sample:
            message = (
                'Inputs to transforms must be instances of torchio.Subject'
                f' generated by a torchio.ImagesDataset, not "{type(sample)}"'
            )
            raise RuntimeError(message)

    def parse_tensor(self, tensor: torch.Tensor) -> Subject:
        num_dimensions = tensor.dim()
        if num_dimensions != 4:
            message = (
                'The input tensor must have 4 dimensions (channels, i, j, k),'
                f' but has {num_dimensions}: {tensor.shape}'
            )
            raise RuntimeError(message)
        return self._get_subject_from_tensor(tensor)

    @staticmethod
    def _get_subject_from_tensor(tensor: torch.Tensor) -> Subject:
        subject_dict = {}
        for channel_index, channel_tensor in enumerate(tensor):
            name = f'channel_{channel_index}'
            image = Image(tensor=channel_tensor, type=INTENSITY)
            subject_dict[name] = image
        subject = Subject(subject_dict)
        dataset = ImagesDataset([subject])
        sample = dataset[0]
        return sample

    @staticmethod
    def nib_to_sitk(data: TypeData, affine: TypeData):
        return nib_to_sitk(data, affine)

    @staticmethod
    def sitk_to_nib(image: sitk.Image):
        return sitk_to_nib(image)

    @staticmethod
    def _fft_im(image):
        output = (np.fft.fftshift(np.fft.fftn(np.fft.ifftshift(image)))).astype(np.complex128)
        return output

    @staticmethod
    def _ifft_im(freq_domain):
        output = np.fft.ifftshift(np.fft.ifftn(freq_domain))
        return output

    @staticmethod
    def _oversample(data, perc_oversampling=.10):
        """
        Oversamples data with a zero padding. Adds perc_oversampling percentage values
        :param data (ndarray): array to pad
        :param perc_oversampling (float): percentage of oversampling to add to data (based on its current shape)
        :return oversampled version of the data:
        """
        data_shape = list(data.shape)
        to_pad = np.ceil(np.asarray(data_shape) * perc_oversampling/2) * 2
        #to force an even number if odd, this will shift the volume when croping
        #print("Pading at {}".format(to_pad))
        left_pad = np.floor(to_pad / 2).astype(int)
        right_pad = np.ceil(to_pad / 2).astype(int)
        return np.pad(data, list(zip(left_pad, right_pad)))

    @staticmethod
    def crop_volume(data, cropping_shape):
        '''
        Cropping data to cropping_shape size. Cropping starts from center of the image
        '''
        vol_centers = (np.asarray(data.shape) / 2).astype(int)
        dim_ranges = np.ceil(np.asarray(cropping_shape) / 2).astype(int)
        slicing = [slice(dim_center - dim_range, dim_center + dim_range)
                   for dim_center, dim_range in zip(vol_centers, dim_ranges)]
        return data[tuple(slicing)]

    @property
    def name(self):
        return self.__class__.__name__
